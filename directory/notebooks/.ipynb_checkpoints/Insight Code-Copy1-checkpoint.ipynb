{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Land Assessment Values in Alberta, Canada**\n",
    "\n",
    "**Dr. Eric Asare**\n",
    "\n",
    "**March 27, 2020**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "A) There is a high cost (both monetary and time) of accessible land assessment value/acre of farmland in Alberta.\n",
    "\n",
    "B) Land assessment value is a principal argument in the function used to estimate the economic value of wetlands.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "A) To estimate a model that could provide accurate predictions of land assessment values in sub-basin 14 in Alberta.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "A) A myriad of supervised models with increasing levels of complexity will be estimated.\n",
    "\n",
    "B) Generic model: Land assessment value  = f(Land feritlity Classes) \n",
    "\n",
    "C) Metric for choosing best model: Coeficient of Determination (R-squared) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "#suppress warnings\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "#for eg after the plotting use save_fig(\"law_of_large_numbers_plot\") here the fig_id is \"law of ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"datasets/finalfinal_june29_5_49pm.csv\", header=None) #already prepared dataset using R\n",
    "dataset = df.values\n",
    "X = dataset[1:,1:16].astype(float)\n",
    "Y = dataset[1:,0].astype(float)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#df.hist(bins=50, figsize=(20,15))\n",
    "#save_fig(\"attribute_histogram_plots\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_reg = LinearRegression()\n",
    "model_lin = lin_reg.fit(X_train,y_train)\n",
    "y_pred_lin = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-square: 0.32018565992570924\n",
      "MAE: 6154.386096342833\n",
      "MSE: 65328948.4846742\n",
      "RMSE: 8082.632521937033\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('R-square:', metrics.r2_score(y_test, y_pred_lin))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred_lin))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred_lin))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_lin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularised LS : Ridge (l2) and Lasso(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-square: 0.3201135895962952\n",
      "MAE: 6155.197508084036\n",
      "MSE: 65335874.31509586\n",
      "RMSE: 8083.060949609118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet #li_ratio =0 ridge; 1 lasso\n",
    "elastic_net = ElasticNet(alpha =0.5 , l1_ratio =1.)\n",
    "model_elas = elastic_net.fit(X_train,y_train)\n",
    "y_pred_elas = elastic_net.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print('R-square:', metrics.r2_score(y_test, y_pred_elas))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred_elas))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred_elas))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_elas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting : Based on Linear regression models and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58 (+/- 0.04) [Random Forest]\n",
      "Accuracy: 0.33 (+/- 0.04) [Lasso]\n",
      "Accuracy: 0.28 (+/- 0.03) [Ridge Regression]\n",
      "Accuracy: 0.46 (+/- 0.03) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ElasticNet_lasso = ElasticNet(alpha = 0.1, l1_ratio =1) #lasso\n",
    "ElasticNet_ridge = ElasticNet(alpha = 0.1, l1_ratio =0) #ridge\n",
    "RndForest_reg = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                      max_depth =50, max_features=9, max_leaf_nodes=None,\n",
    "                      max_samples=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=3,\n",
    "                      min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                      random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "clf_voting = [ElasticNet_lasso,ElasticNet_ridge]\n",
    "\n",
    "eclf = VotingRegressor(estimators=[('Random Forests', RndForest_reg), \n",
    "                                   ('Lasso', ElasticNet_lasso), ('Ridge Regression', ElasticNet_ridge)])\n",
    "for clf, label in zip([RndForest_reg,ElasticNet_lasso,ElasticNet_ridge, eclf], \n",
    "                      ['Random Forest', 'Lasso', 'Ridge Regression', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='r2')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "# A) Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.227, std: (+/-) 0.079 [Adaboost]\n",
      "Mean: 0.245, std: (+/-) 0.080 [gradient]\n",
      "Mean: 0.384, std: (+/-) 0.035 [xgb]\n",
      "Mean: 0.462, std: (+/-) 0.035 [votingre]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ada_boost = AdaBoostRegressor()\n",
    "grad_boost = AdaBoostRegressor()\n",
    "xgb_boost = xgboost.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 1, learning_rate = 1,\n",
    "                max_depth = 12, alpha = 1, n_estimators = 400)\n",
    "\n",
    "ereg = VotingRegressor(estimators=[ada_boost, grad_boost, xgb_boost])\n",
    "\n",
    "#ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "\n",
    "labels = [\"Adaboost\", \"gradient\", \"xgb\", \"votingre\"]\n",
    "\n",
    "boost_array = [ada_boost, grad_boost, xgb_boost]\n",
    "\n",
    "for clf, label in zip([ada_boost, grad_boost, xgb_boost, eclf], labels):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='r2')\n",
    "    print(\"Mean: {0:.3f}, std: (+/-) {1:.3f} [{2}]\".format(scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of: 0.577, std: (+/-) 0.038 [RandomForestRegressor]\n",
      "Mean of: 0.516, std: (+/-) 0.034 [Bagging RandomForestRegressor]\n",
      "\n",
      "Mean of: -0.001, std: (+/-) 0.001 [SVR]\n",
      "Mean of: -0.000, std: (+/-) 0.002 [Bagging SVR]\n",
      "\n",
      "Mean of: -0.853, std: (+/-) 1.516 [LinearSVR]\n",
      "Mean of: -0.161, std: (+/-) 0.219 [Bagging LinearSVR]\n",
      "\n",
      "Mean of: 0.327, std: (+/-) 0.038 [ElasticNet]\n",
      "Mean of: 0.310, std: (+/-) 0.032 [Bagging ElasticNet]\n",
      "\n",
      "Mean of: 0.285, std: (+/-) 0.027 [ElasticNet]\n",
      "Mean of: 0.250, std: (+/-) 0.024 [Bagging ElasticNet]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import ElasticNet #li_ratio =0 ridge; 1 lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "seed = 1075\n",
    "np.random.seed(seed)\n",
    "\n",
    "SVR_reg = SVR()\n",
    "SVR_lin = LinearSVR(epsilon=0.7, C=100)\n",
    "ElasticNet_lasso = ElasticNet(alpha = 0.1, l1_ratio =1) #lasso\n",
    "ElasticNet_ridge = ElasticNet(alpha = 0.1, l1_ratio =0) #ridge\n",
    "RndForest_reg = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "                      max_depth =50, max_features=9, max_leaf_nodes=None,\n",
    "                      max_samples=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=3,\n",
    "                      min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                      random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "clf_bagging = [RndForest_reg, SVR_reg, SVR_lin, ElasticNet_lasso,ElasticNet_ridge]\n",
    "\n",
    "for clf in clf_bagging:\n",
    "    non_bagging_score = cross_val_score(clf, X_train, y_train, cv=10, n_jobs=-1)\n",
    "    bagging_reg = BaggingRegressor(clf,max_samples=0.4, max_features=10, random_state=seed)\n",
    "    bagging_scores = cross_val_score(bagging_reg, X_train, y_train, cv=10,n_jobs=-1,  scoring='r2')\n",
    "    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [{0}]\".format(clf.__class__.__name__,non_bagging_score.mean(),\n",
    "                                                              non_bagging_score.std()))\n",
    "    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\".format(clf.__class__.__name__,\n",
    "                                                                         bagging_scores.mean(), bagging_scores.std()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "#param_grid = {\n",
    "   # 'bootstrap': [True],\n",
    "   # 'max_depth': [80, 90, 100, 110, 200, 300, 400],\n",
    "  #  'max_features': [2, 3],\n",
    "  #  'min_samples_leaf': [3, 4, 5],\n",
    "  #  'min_samples_split': [8, 10, 12],\n",
    "   # 'n_estimators': [100, 200, 300, 1000]\n",
    "#}\n",
    "# Create a based model\n",
    "#rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "#grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "#grid_search.fit(X, Y)\n",
    "#best_gri = grid_search.best_estimator_\n",
    "#best_gri\n",
    "#params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "             #   'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "#xgb_reg.cv(X_train, y_train, params=params, nfold=3,\n",
    "                   # num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Results\n",
    "\n",
    "A) Random forest has the predictive power based on r-squared.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Works\n",
    "A) Gather more variables.\n",
    "\n",
    "B) Random forest will be explored further using grid searching of parameter space to improve on the results.\n",
    "\n",
    "B) Python GUI Application with Tkinter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
